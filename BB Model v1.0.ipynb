{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  55873682\n",
      "json load time:  4.183709144592285\n",
      "total time:  26.842750549316406\n"
     ]
    }
   ],
   "source": [
    "m1 = time.time()\n",
    "with open(\"bb_train<20.json\") as f:\n",
    "    data=json.load(f)\n",
    "print('num samples: ',len(data))\n",
    "print('json load time: ',time.time()-m1)\n",
    "\n",
    "#cant be too large, will trigger a memory error later on. Failed at 1 Mill\n",
    "np.random.shuffle(data)\n",
    "data = data[:1000000]\n",
    "\n",
    "print('total time: ',time.time()-m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to optimize this later, use np arrays instead of python lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaspted text->int seqs:  39.57601809501648\n",
      "total time:  41.54301404953003\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Need to optimize this later, replace use np arrays \n",
    "instead of python lists\n",
    "\"\"\"\n",
    "top_k = 10000\n",
    "\n",
    "t = Tokenizer(num_words=top_k, oov_token='<unk>')\n",
    "t.fit_on_texts(data)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "seqs = [t.texts_to_sequences(x) for x in data]\n",
    "print('elaspted text->int seqs: ',time.time() - start)\n",
    "\n",
    "seq_data = list()\n",
    "for seq in seqs:\n",
    "    flat = [num for sublist in seq for num in sublist]\n",
    "    seq_data.append(flat)\n",
    "    \n",
    "print('total time: ',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fcb84c79240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, seqs took 57s to go through 1 mill samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Len: 19\n",
      "vocab sz:  40497\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(seq) for seq in seq_data])\n",
    "\n",
    "sequences = pad_sequences(seq_data, maxlen=max_len, padding='pre')\n",
    "\n",
    "print('Max Seq Len: %d' % max_len)\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "vocab_sz = len(t.word_index)+1\n",
    "print('vocab sz: ',vocab_sz)\n",
    "\n",
    "#y_cat = to_categorical(y, num_classes=vocab_sz) #1-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           12149100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40497)             2065347   \n",
      "=================================================================\n",
      "Total params: 14,284,647\n",
      "Trainable params: 14,284,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=7)\n",
    "\n",
    "#NN definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sz, 300, input_length=max_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_sz, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#save model structure as json\n",
    "\n",
    "#model_json = model.to_json()\n",
    "#with open(\"models/huffpo-model-v1.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 850000 samples, validate on 150000 samples\n",
      "Epoch 1/20\n",
      "850000/850000 [==============================] - 701s 825us/step - loss: 0.8093 - acc: 0.7548 - val_loss: 0.7712 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77118, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 2/20\n",
      "850000/850000 [==============================] - 700s 823us/step - loss: 0.7269 - acc: 0.7782 - val_loss: 0.7078 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77118 to 0.70785, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 3/20\n",
      "850000/850000 [==============================] - 693s 816us/step - loss: 0.6867 - acc: 0.7887 - val_loss: 0.6755 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70785 to 0.67555, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 4/20\n",
      "850000/850000 [==============================] - 693s 816us/step - loss: 0.6613 - acc: 0.7962 - val_loss: 0.6510 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67555 to 0.65102, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 5/20\n",
      "850000/850000 [==============================] - 694s 816us/step - loss: 0.6433 - acc: 0.8010 - val_loss: 0.6397 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65102 to 0.63971, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 6/20\n",
      "850000/850000 [==============================] - 699s 822us/step - loss: 0.6281 - acc: 0.8053 - val_loss: 0.6327 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.63971 to 0.63274, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 7/20\n",
      "850000/850000 [==============================] - 694s 817us/step - loss: 0.6181 - acc: 0.8083 - val_loss: 0.6204 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.63274 to 0.62039, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 8/20\n",
      "850000/850000 [==============================] - 693s 815us/step - loss: 0.6099 - acc: 0.8107 - val_loss: 0.6159 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.62039 to 0.61587, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 9/20\n",
      "850000/850000 [==============================] - 697s 819us/step - loss: 0.6020 - acc: 0.8132 - val_loss: 0.6058 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61587 to 0.60580, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 10/20\n",
      "850000/850000 [==============================] - 691s 813us/step - loss: 0.5970 - acc: 0.8144 - val_loss: 0.6010 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60580 to 0.60098, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 11/20\n",
      "850000/850000 [==============================] - 735s 865us/step - loss: 0.5922 - acc: 0.8159 - val_loss: 0.6075 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.60098\n",
      "Epoch 12/20\n",
      "850000/850000 [==============================] - 726s 854us/step - loss: 0.5876 - acc: 0.8172 - val_loss: 0.5923 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60098 to 0.59226, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 13/20\n",
      "850000/850000 [==============================] - 697s 820us/step - loss: 0.5835 - acc: 0.8178 - val_loss: 0.5919 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.59226 to 0.59190, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 14/20\n",
      "850000/850000 [==============================] - 699s 822us/step - loss: 0.5812 - acc: 0.8187 - val_loss: 0.5867 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.59190 to 0.58672, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 15/20\n",
      "850000/850000 [==============================] - 700s 824us/step - loss: 0.5788 - acc: 0.8192 - val_loss: 0.5787 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.58672 to 0.57873, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 16/20\n",
      "850000/850000 [==============================] - 694s 817us/step - loss: 0.5741 - acc: 0.8203 - val_loss: 0.6195 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.57873\n",
      "Epoch 17/20\n",
      "850000/850000 [==============================] - 697s 820us/step - loss: 0.5782 - acc: 0.8202 - val_loss: 0.5834 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.57873\n",
      "Epoch 18/20\n",
      "850000/850000 [==============================] - 693s 816us/step - loss: 0.5724 - acc: 0.8214 - val_loss: 0.5793 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.57873\n",
      "Epoch 19/20\n",
      "850000/850000 [==============================] - 697s 820us/step - loss: 0.5689 - acc: 0.8220 - val_loss: 0.5763 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.57873 to 0.57631, saving model to weights/bb-v1/best-weights.hdf5\n",
      "Epoch 20/20\n",
      "850000/850000 [==============================] - 690s 812us/step - loss: 0.5676 - acc: 0.8226 - val_loss: 0.5750 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.57631 to 0.57501, saving model to weights/bb-v1/best-weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95643169e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/bb-model-v1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "fpath = \"weights/bb-v1/best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fpath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "BS = 42\n",
    "tb = TensorBoard(log_dir=\"tensorboard-logs/{}\".format(time.time()))\n",
    "callback_lst = [checkpoint, tb]\n",
    "#steps_per_epoch is num of batches that make up 1 epoch, defaults to size of train set\n",
    "model.fit(X,y,batch_size=BS, validation_split=.15, epochs=20, callbacks=callback_lst, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('bb-v1-tok.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_hp_comments.json') as json_file:  #first 1mill bb comments\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hp_tok = Tokenizer(oov_token='<unk>')\n",
    "hp_tok.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hp_tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp-v5-tok.pickle', 'wb') as handle:\n",
    "    pickle.dump(hp_tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = data\n",
    "\n",
    "seqs = hp_tok.texts_to_sequences(train_data)\n",
    "less_20_seqs = list(filter(lambda x: len(x) <20, seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705009"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(less_20_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Len: 19\n"
     ]
    }
   ],
   "source": [
    "MAXLEN = 19\n",
    "VOCAB = int(len(hp_tok.word_index))\n",
    "SEED = 42\n",
    "\n",
    "sequences = pad_sequences(less_20_seqs, maxlen=MAXLEN, padding='pre')\n",
    "\n",
    "print('Max Seq Len: %d' % MAXLEN)\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=SEED)\n",
    "\n",
    "\n",
    "#y_cat = to_categorical(y, num_classes=vocab_sz) #1-hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to optimize this later, use np arrays instead of python lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, seqs took 57s to go through 1 mill samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           3000600   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500)               1602000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10002)             5011002   \n",
      "=================================================================\n",
      "Total params: 9,613,602\n",
      "Trainable params: 9,613,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#NN definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB, 300, input_length=MAXLEN-1))\n",
    "model.add(LSTM(500))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(VOCAB, activation='softmax'))\n",
    "model.load_weights('weights/huffpo-v5/baseline-weights.hdf5')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 593264 samples, validate on 5993 samples\n",
      "Epoch 1/200\n",
      "593264/593264 [==============================] - 159s 267us/step - loss: 3.7635 - acc: 0.3305 - val_loss: 4.8890 - val_acc: 0.2833\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.33052, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 2/200\n",
      "593264/593264 [==============================] - 159s 268us/step - loss: 3.3548 - acc: 0.3628 - val_loss: 5.0220 - val_acc: 0.2827\n",
      "\n",
      "Epoch 00002: acc improved from 0.33052 to 0.36277, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 3/200\n",
      "593264/593264 [==============================] - 159s 267us/step - loss: 3.0683 - acc: 0.3987 - val_loss: 5.1428 - val_acc: 0.2703\n",
      "\n",
      "Epoch 00003: acc improved from 0.36277 to 0.39867, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 4/200\n",
      "593264/593264 [==============================] - 158s 266us/step - loss: 2.8372 - acc: 0.4325 - val_loss: 5.2508 - val_acc: 0.2757\n",
      "\n",
      "Epoch 00004: acc improved from 0.39867 to 0.43253, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 5/200\n",
      "593264/593264 [==============================] - 162s 273us/step - loss: 2.6468 - acc: 0.4632 - val_loss: 5.3720 - val_acc: 0.2706\n",
      "\n",
      "Epoch 00005: acc improved from 0.43253 to 0.46321, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 6/200\n",
      "593264/593264 [==============================] - 161s 272us/step - loss: 2.4848 - acc: 0.4913 - val_loss: 5.4660 - val_acc: 0.2591\n",
      "\n",
      "Epoch 00006: acc improved from 0.46321 to 0.49128, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 7/200\n",
      "593264/593264 [==============================] - 183s 308us/step - loss: 2.3509 - acc: 0.5152 - val_loss: 5.5601 - val_acc: 0.2623\n",
      "\n",
      "Epoch 00007: acc improved from 0.49128 to 0.51518, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 8/200\n",
      "593264/593264 [==============================] - 253s 426us/step - loss: 2.2377 - acc: 0.5349 - val_loss: 5.6920 - val_acc: 0.2596\n",
      "\n",
      "Epoch 00008: acc improved from 0.51518 to 0.53491, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 9/200\n",
      "593264/593264 [==============================] - 253s 427us/step - loss: 2.1438 - acc: 0.5524 - val_loss: 5.7534 - val_acc: 0.2570\n",
      "\n",
      "Epoch 00009: acc improved from 0.53491 to 0.55243, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 10/200\n",
      "593264/593264 [==============================] - 345s 582us/step - loss: 2.0634 - acc: 0.5673 - val_loss: 5.8761 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00010: acc improved from 0.55243 to 0.56726, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 11/200\n",
      "593264/593264 [==============================] - 468s 789us/step - loss: 1.9971 - acc: 0.5798 - val_loss: 5.9568 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00011: acc improved from 0.56726 to 0.57979, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 12/200\n",
      "593264/593264 [==============================] - 487s 821us/step - loss: 1.9444 - acc: 0.5893 - val_loss: 6.0170 - val_acc: 0.2550\n",
      "\n",
      "Epoch 00012: acc improved from 0.57979 to 0.58932, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 13/200\n",
      "593264/593264 [==============================] - 492s 830us/step - loss: 1.8988 - acc: 0.5975 - val_loss: 6.1208 - val_acc: 0.2560\n",
      "\n",
      "Epoch 00013: acc improved from 0.58932 to 0.59748, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 14/200\n",
      "593264/593264 [==============================] - 526s 886us/step - loss: 1.8612 - acc: 0.6043 - val_loss: 6.1814 - val_acc: 0.2488\n",
      "\n",
      "Epoch 00014: acc improved from 0.59748 to 0.60430, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 15/200\n",
      "593264/593264 [==============================] - 518s 874us/step - loss: 1.8310 - acc: 0.6094 - val_loss: 6.2408 - val_acc: 0.2458\n",
      "\n",
      "Epoch 00015: acc improved from 0.60430 to 0.60940, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 16/200\n",
      "593264/593264 [==============================] - 511s 861us/step - loss: 1.8045 - acc: 0.6141 - val_loss: 6.2934 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00016: acc improved from 0.60940 to 0.61407, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 17/200\n",
      "593264/593264 [==============================] - 518s 874us/step - loss: 1.7849 - acc: 0.6171 - val_loss: 6.3661 - val_acc: 0.2445\n",
      "\n",
      "Epoch 00017: acc improved from 0.61407 to 0.61706, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 18/200\n",
      "593264/593264 [==============================] - 489s 824us/step - loss: 1.7657 - acc: 0.6204 - val_loss: 6.4101 - val_acc: 0.2453\n",
      "\n",
      "Epoch 00018: acc improved from 0.61706 to 0.62039, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 19/200\n",
      "593264/593264 [==============================] - 489s 824us/step - loss: 1.7535 - acc: 0.6217 - val_loss: 6.4589 - val_acc: 0.2456\n",
      "\n",
      "Epoch 00019: acc improved from 0.62039 to 0.62167, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 20/200\n",
      "593264/593264 [==============================] - 553s 931us/step - loss: 1.7464 - acc: 0.6222 - val_loss: 6.4938 - val_acc: 0.2404\n",
      "\n",
      "Epoch 00020: acc improved from 0.62167 to 0.62219, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 21/200\n",
      "593264/593264 [==============================] - 559s 943us/step - loss: 1.7374 - acc: 0.6233 - val_loss: 6.5192 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00021: acc improved from 0.62219 to 0.62327, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 22/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7325 - acc: 0.6235 - val_loss: 6.5799 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00022: acc improved from 0.62327 to 0.62347, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 23/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7265 - acc: 0.6237 - val_loss: 6.6020 - val_acc: 0.2403\n",
      "\n",
      "Epoch 00023: acc improved from 0.62347 to 0.62371, saving model to weights/huffpo-v5/best-weights.hdf5\n",
      "Epoch 24/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7258 - acc: 0.6230 - val_loss: 6.6388 - val_acc: 0.2363\n",
      "\n",
      "Epoch 00024: acc did not improve from 0.62371\n",
      "Epoch 25/200\n",
      "593264/593264 [==============================] - 559s 941us/step - loss: 1.7230 - acc: 0.6232 - val_loss: 6.6637 - val_acc: 0.2418\n",
      "\n",
      "Epoch 00025: acc did not improve from 0.62371\n",
      "Epoch 26/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7231 - acc: 0.6221 - val_loss: 6.7116 - val_acc: 0.2414\n",
      "\n",
      "Epoch 00026: acc did not improve from 0.62371\n",
      "Epoch 27/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7188 - acc: 0.6227 - val_loss: 6.7132 - val_acc: 0.2348\n",
      "\n",
      "Epoch 00027: acc did not improve from 0.62371\n",
      "Epoch 28/200\n",
      "593264/593264 [==============================] - 559s 942us/step - loss: 1.7207 - acc: 0.6217 - val_loss: 6.7748 - val_acc: 0.2363\n",
      "\n",
      "Epoch 00028: acc did not improve from 0.62371\n",
      "Epoch 29/200\n",
      "593264/593264 [==============================] - 557s 940us/step - loss: 1.7252 - acc: 0.6202 - val_loss: 6.7863 - val_acc: 0.2359\n",
      "\n",
      "Epoch 00029: acc did not improve from 0.62371\n",
      "Epoch 30/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7237 - acc: 0.6198 - val_loss: 6.8057 - val_acc: 0.2398\n",
      "\n",
      "Epoch 00030: acc did not improve from 0.62371\n",
      "Epoch 31/200\n",
      "593264/593264 [==============================] - 558s 941us/step - loss: 1.7298 - acc: 0.6179 - val_loss: 6.8106 - val_acc: 0.2398\n",
      "\n",
      "Epoch 00031: acc did not improve from 0.62371\n",
      "Epoch 32/200\n",
      "593264/593264 [==============================] - 557s 939us/step - loss: 1.7259 - acc: 0.6185 - val_loss: 6.8180 - val_acc: 0.2398\n",
      "\n",
      "Epoch 00032: acc did not improve from 0.62371\n",
      "Epoch 33/200\n",
      "593264/593264 [==============================] - 559s 943us/step - loss: 1.7352 - acc: 0.6159 - val_loss: 6.8367 - val_acc: 0.2358\n",
      "\n",
      "Epoch 00033: acc did not improve from 0.62371\n",
      "Epoch 34/200\n",
      " 44300/593264 [=>............................] - ETA: 8:37 - loss: 1.5479 - acc: 0.6614"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/hp-model-v5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "fpath = \"weights/huffpo-v5/best-weights.hdf5\" #off of training acc\n",
    "checkpoint = ModelCheckpoint(fpath, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "BS = 100\n",
    "tb = TensorBoard(log_dir=\"tensorboard-logs/{}\".format('huffpo-modelv5-run2'))\n",
    "callback_lst = [checkpoint, tb]\n",
    "#steps_per_epoch is num of batches that make up 1 epoch, defaults to size of train set\n",
    "model.fit(X_train,y_train,batch_size=BS, validation_split=.01, epochs=200, callbacks=callback_lst, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"huffpo_train<20.json\") as f: #all huffpo comments of length <20 words\n",
    "    data=json.load(f)\n",
    "sample = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gia',\n",
       " 'mapp',\n",
       " 'that',\n",
       " 'nags',\n",
       " 'the',\n",
       " 'hell',\n",
       " 'out',\n",
       " 'of',\n",
       " 'me',\n",
       " 'christians',\n",
       " 'what',\n",
       " 'bible',\n",
       " 'are',\n",
       " 'they',\n",
       " 'reading']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(oov_token='<unk>')\n",
    "t.fit_on_texts(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_counts',\n",
       " 'word_docs',\n",
       " 'filters',\n",
       " 'split',\n",
       " 'lower',\n",
       " 'num_words',\n",
       " 'document_count',\n",
       " 'char_level',\n",
       " 'oov_token',\n",
       " 'index_docs',\n",
       " 'word_index',\n",
       " 'index_word',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " 'fit_on_texts',\n",
       " 'fit_on_sequences',\n",
       " 'texts_to_sequences',\n",
       " 'texts_to_sequences_generator',\n",
       " 'sequences_to_texts',\n",
       " 'sequences_to_texts_generator',\n",
       " 'texts_to_matrix',\n",
       " 'sequences_to_matrix',\n",
       " 'get_config',\n",
       " 'to_json',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__dir__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.texts_to_sequences_generator(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_sample = list(map(lambda comment: t.texts_to_sequences(comment), sample))\n",
    "seq_sample = [t.texts_to_sequences(x) for x in sample]\n",
    "seq_data = list()\n",
    "for seq in seq_sample:\n",
    "    flat = [num for sublist in seq for num in sublist]\n",
    "    seq_data.append(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 420, 587, 58, 128, 258, 320, 22, 45, 421, 48, 1035, 1036, 1037]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gia mapp that nags the hell out of me christians what bible are they reading',\n",
       " 'this is starting to be an annoying bandwagon now']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sequences_to_texts(seq_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 19\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(seq) for seq in seq_data])\n",
    "sequences = pad_sequences(seq_data, maxlen=max_length, padding='pre')\n",
    "\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 1029, 1030,   10, 1031,    2,  319,   47,\n",
       "          9,   68,  586,   27, 1032,   15,   22,  257], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 3003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "vocab_sz = len(t.word_index)+1\n",
    "print('vocab:',vocab_sz)\n",
    "y_cat = to_categorical(y, num_classes=vocab_sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]->[   0    0    0    0 1029 1030   10 1031    2  319   47    9   68  586\n",
      "   27 1032   15   22]\n",
      "y[0]->257\n",
      "y_cat[0]->[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"X[0]->{}\\ny[0]->{}\\ny_cat[0]->{}\".format(X[0],y[0],y_cat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1].[   0    0    0    0    0    0    0    0    0    0   23    6  419    5\n",
      "   18   34 1033 1034].\n",
      "y[1].90.\n",
      "y_cat[1].[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"X[1].{}.\\ny[1].{}.\\ny_cat[1].{}\".format(X[1],y[1],y_cat[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3003)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3003\n"
     ]
    }
   ],
   "source": [
    "print(vocab_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 100)           300300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3003)              153153    \n",
      "=================================================================\n",
      "Total params: 483,653\n",
      "Trainable params: 483,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sz, 100, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_sz, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 4s - loss: 7.8946 - acc: 0.0200\n",
      "Epoch 2/100\n",
      " - 3s - loss: 6.8445 - acc: 0.0230\n",
      "Epoch 3/100\n",
      " - 3s - loss: 6.5276 - acc: 0.0230\n",
      "Epoch 4/100\n",
      " - 3s - loss: 6.3786 - acc: 0.0230\n",
      "Epoch 5/100\n",
      " - 3s - loss: 6.2414 - acc: 0.0230\n",
      "Epoch 6/100\n",
      " - 3s - loss: 6.1055 - acc: 0.0230\n",
      "Epoch 7/100\n",
      " - 3s - loss: 5.9724 - acc: 0.0230\n",
      "Epoch 8/100\n",
      " - 3s - loss: 5.8498 - acc: 0.0310\n",
      "Epoch 9/100\n",
      " - 3s - loss: 5.7258 - acc: 0.0360\n",
      "Epoch 10/100\n",
      " - 3s - loss: 5.5993 - acc: 0.0340\n",
      "Epoch 11/100\n",
      " - 3s - loss: 5.4632 - acc: 0.0380\n",
      "Epoch 12/100\n",
      " - 3s - loss: 5.3329 - acc: 0.0460\n",
      "Epoch 13/100\n",
      " - 3s - loss: 5.1933 - acc: 0.0520\n",
      "Epoch 14/100\n",
      " - 3s - loss: 5.0560 - acc: 0.0560\n",
      "Epoch 15/100\n",
      " - 3s - loss: 4.9220 - acc: 0.0640\n",
      "Epoch 16/100\n",
      " - 3s - loss: 4.7952 - acc: 0.0670\n",
      "Epoch 17/100\n",
      " - 3s - loss: 4.6614 - acc: 0.0870\n",
      "Epoch 18/100\n",
      " - 3s - loss: 4.5324 - acc: 0.1020\n",
      "Epoch 19/100\n",
      " - 3s - loss: 4.4066 - acc: 0.1170\n",
      "Epoch 20/100\n",
      " - 3s - loss: 4.2818 - acc: 0.1500\n",
      "Epoch 21/100\n",
      " - 3s - loss: 4.1611 - acc: 0.1830\n",
      "Epoch 22/100\n",
      " - 3s - loss: 4.0248 - acc: 0.2090\n",
      "Epoch 23/100\n",
      " - 3s - loss: 3.8923 - acc: 0.2620\n",
      "Epoch 24/100\n",
      " - 3s - loss: 3.7688 - acc: 0.3040\n",
      "Epoch 25/100\n",
      " - 3s - loss: 3.6411 - acc: 0.3680\n",
      "Epoch 26/100\n",
      " - 3s - loss: 3.5245 - acc: 0.4070\n",
      "Epoch 27/100\n",
      " - 3s - loss: 3.3962 - acc: 0.4520\n",
      "Epoch 28/100\n",
      " - 3s - loss: 3.2732 - acc: 0.4860\n",
      "Epoch 29/100\n",
      " - 3s - loss: 3.1493 - acc: 0.5370\n",
      "Epoch 30/100\n",
      " - 3s - loss: 3.0267 - acc: 0.5880\n",
      "Epoch 31/100\n",
      " - 3s - loss: 2.9039 - acc: 0.6120\n",
      "Epoch 32/100\n",
      " - 3s - loss: 2.7937 - acc: 0.6660\n",
      "Epoch 33/100\n",
      " - 3s - loss: 2.6742 - acc: 0.6880\n",
      "Epoch 34/100\n",
      " - 3s - loss: 2.5624 - acc: 0.7230\n",
      "Epoch 35/100\n",
      " - 3s - loss: 2.4469 - acc: 0.7530\n",
      "Epoch 36/100\n",
      " - 3s - loss: 2.3394 - acc: 0.7790\n",
      "Epoch 37/100\n",
      " - 3s - loss: 2.2314 - acc: 0.8020\n",
      "Epoch 38/100\n",
      " - 3s - loss: 2.1277 - acc: 0.8240\n",
      "Epoch 39/100\n",
      " - 3s - loss: 2.0311 - acc: 0.8320\n",
      "Epoch 40/100\n",
      " - 3s - loss: 1.9343 - acc: 0.8540\n",
      "Epoch 41/100\n",
      " - 3s - loss: 1.8438 - acc: 0.8700\n",
      "Epoch 42/100\n",
      " - 3s - loss: 1.7497 - acc: 0.8720\n",
      "Epoch 43/100\n",
      " - 3s - loss: 1.6606 - acc: 0.8940\n",
      "Epoch 44/100\n",
      " - 3s - loss: 1.5946 - acc: 0.9030\n",
      "Epoch 45/100\n",
      " - 3s - loss: 1.5117 - acc: 0.9000\n",
      "Epoch 46/100\n",
      " - 3s - loss: 1.4338 - acc: 0.9100\n",
      "Epoch 47/100\n",
      " - 3s - loss: 1.3609 - acc: 0.9150\n",
      "Epoch 48/100\n",
      " - 3s - loss: 1.2827 - acc: 0.9250\n",
      "Epoch 49/100\n",
      " - 3s - loss: 1.2167 - acc: 0.9340\n",
      "Epoch 50/100\n",
      " - 3s - loss: 1.1591 - acc: 0.9350\n",
      "Epoch 51/100\n",
      " - 3s - loss: 1.1078 - acc: 0.9430\n",
      "Epoch 52/100\n",
      " - 3s - loss: 1.0529 - acc: 0.9350\n",
      "Epoch 53/100\n",
      " - 3s - loss: 1.0067 - acc: 0.9390\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.9444 - acc: 0.9440\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.8899 - acc: 0.9480\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.8420 - acc: 0.9550\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.7972 - acc: 0.9580\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.7574 - acc: 0.9560\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.7193 - acc: 0.9570\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.6857 - acc: 0.9600\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.6523 - acc: 0.9640\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.6211 - acc: 0.9630\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.5910 - acc: 0.9670\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.5644 - acc: 0.9630\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.5427 - acc: 0.9670\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.5178 - acc: 0.9680\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.4913 - acc: 0.9670\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.4707 - acc: 0.9700\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.4506 - acc: 0.9690\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.4323 - acc: 0.9670\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.4145 - acc: 0.9690\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.3976 - acc: 0.9710\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.3969 - acc: 0.9700\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.3835 - acc: 0.9710\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.3648 - acc: 0.9710\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.3540 - acc: 0.9710\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.3364 - acc: 0.9700\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.3171 - acc: 0.9710\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.3054 - acc: 0.9720\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.2946 - acc: 0.9710\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.2856 - acc: 0.9720\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.2764 - acc: 0.9720\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.2685 - acc: 0.9710\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.2594 - acc: 0.9720\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.2534 - acc: 0.9720\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.2464 - acc: 0.9730\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.2395 - acc: 0.9720\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.2333 - acc: 0.9730\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.2276 - acc: 0.9730\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.2223 - acc: 0.9720\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.2163 - acc: 0.9730\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.2115 - acc: 0.9730\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.2076 - acc: 0.9730\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.2029 - acc: 0.9720\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.1984 - acc: 0.9730\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.1947 - acc: 0.9730\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.1906 - acc: 0.9720\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.1879 - acc: 0.9740\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.1867 - acc: 0.9740\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.1818 - acc: 0.9730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff334203240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "\n",
    "if (X.ndim == 1):\n",
    "    X = numpy.array([X])\n",
    "    \n",
    "model.fit(X, y_cat,batch_size=10, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "him much retarded cheat cheat proud proud money china hahaha lmfao marriage charlottesville a picture honor insane insane snicker bud snicker\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, t, max_length-1, 'him', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump is the worst research time ago ago ago ago ago there him him like like right right right it please hahahahah disabled touch\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, t, max_length-1, 'trump is the worst', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

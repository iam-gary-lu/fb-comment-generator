{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  55873682\n",
      "elaspted text->int seqs:  46.21088671684265\n",
      "total seq construction time:  48.28778028488159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"bb_train<20.json\") as f:\n",
    "    all_data=json.load(f)\n",
    "print('num samples: ',len(all_data))\n",
    "\n",
    "\n",
    "#cant be too large, will trigger a memory error later on. Failed at 1 Mill\n",
    "#np.random.shuffle(all_data)\n",
    "\n",
    "data = all_data[:1000000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Need to optimize this later, replace use np arrays \n",
    "#instead of python lists\n",
    "\n",
    "#top_k = 10000\n",
    "\n",
    "t = Tokenizer(oov_token='<unk>')\n",
    "t.fit_on_texts(data)\n",
    "\n",
    "\n",
    "with open('bb-v3.1-tok.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "seqs = [t.texts_to_sequences(x) for x in data]\n",
    "print('elaspted text->int seqs: ',time.time() - start)\n",
    "\n",
    "seq_data = list()\n",
    "for seq in seqs:\n",
    "    flat = [num for sublist in seq for num in sublist]\n",
    "    seq_data.append(flat)\n",
    "    \n",
    "print('total seq construction time: ',time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to optimize this later, use np arrays instead of python lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, seqs took 57s to go through 1 mill samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Len: 19\n",
      "vocab sz:  41031\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(seq) for seq in seq_data])\n",
    "\n",
    "sequences = pad_sequences(seq_data, maxlen=max_len, padding='pre')\n",
    "\n",
    "print('Max Seq Len: %d' % max_len)\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "vocab_sz = len(t.word_index)+1\n",
    "print('vocab sz: ',vocab_sz)\n",
    "\n",
    "#y_cat = to_categorical(y, num_classes=vocab_sz) #1-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           12309300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500)               1602000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41031)             20556531  \n",
      "=================================================================\n",
      "Total params: 34,467,831\n",
      "Trainable params: 34,467,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=7)\n",
    "\n",
    "#NN definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sz, 300, input_length=max_len-1))\n",
    "model.add(LSTM(500))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(vocab_sz, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#save model structure as json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/huffpo-model-v3.1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 995000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "995000/995000 [==============================] - 542s 545us/step - loss: 2.6106 - acc: 0.2412 - val_loss: 2.5350 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.25400, saving model to weights/bb-v3.1/best-weights.hdf5\n",
      "Epoch 2/200\n",
      "995000/995000 [==============================] - 537s 540us/step - loss: 2.5656 - acc: 0.2436 - val_loss: 2.5147 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.25400 to 0.25840, saving model to weights/bb-v3.1/best-weights.hdf5\n",
      "Epoch 3/200\n",
      "995000/995000 [==============================] - 540s 542us/step - loss: 2.5641 - acc: 0.2440 - val_loss: 2.5304 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.25840\n",
      "Epoch 4/200\n",
      "995000/995000 [==============================] - 539s 541us/step - loss: 2.5637 - acc: 0.2438 - val_loss: 2.5269 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.25840\n",
      "Epoch 5/200\n",
      "995000/995000 [==============================] - 540s 543us/step - loss: 2.5636 - acc: 0.2438 - val_loss: 2.5195 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.25840\n",
      "Epoch 6/200\n",
      "995000/995000 [==============================] - 538s 541us/step - loss: 2.5643 - acc: 0.2436 - val_loss: 2.5268 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.25840\n",
      "Epoch 7/200\n",
      "995000/995000 [==============================] - 539s 542us/step - loss: 2.5643 - acc: 0.2436 - val_loss: 2.5090 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.25840\n",
      "Epoch 8/200\n",
      "995000/995000 [==============================] - 538s 541us/step - loss: 2.5638 - acc: 0.2439 - val_loss: 2.5315 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.25840\n",
      "Epoch 9/200\n",
      "995000/995000 [==============================] - 540s 542us/step - loss: 2.5641 - acc: 0.2437 - val_loss: 2.5200 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.25840\n",
      "Epoch 10/200\n",
      "995000/995000 [==============================] - 538s 540us/step - loss: 2.5636 - acc: 0.2439 - val_loss: 2.5512 - val_acc: 0.2148\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.25840\n",
      "Epoch 11/200\n",
      "995000/995000 [==============================] - 538s 540us/step - loss: 2.5643 - acc: 0.2436 - val_loss: 2.5206 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.25840\n",
      "Epoch 12/200\n",
      "995000/995000 [==============================] - 539s 541us/step - loss: 2.5640 - acc: 0.2434 - val_loss: 2.5502 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.25840\n",
      "Epoch 13/200\n",
      "995000/995000 [==============================] - 539s 541us/step - loss: 2.5635 - acc: 0.2438 - val_loss: 2.5540 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.25840\n",
      "Epoch 14/200\n",
      "995000/995000 [==============================] - 540s 543us/step - loss: 2.5648 - acc: 0.2430 - val_loss: 2.5706 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.25840\n",
      "Epoch 15/200\n",
      "995000/995000 [==============================] - 539s 542us/step - loss: 2.5645 - acc: 0.2434 - val_loss: 2.5446 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.25840\n",
      "Epoch 16/200\n",
      "995000/995000 [==============================] - 539s 542us/step - loss: 2.5634 - acc: 0.2439 - val_loss: 2.5493 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.25840\n",
      "Epoch 17/200\n",
      "995000/995000 [==============================] - 545s 548us/step - loss: 2.5641 - acc: 0.2435 - val_loss: 2.5485 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.25840\n",
      "Epoch 18/200\n",
      "995000/995000 [==============================] - 539s 541us/step - loss: 2.5643 - acc: 0.2434 - val_loss: 2.5248 - val_acc: 0.2394\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.25840\n",
      "Epoch 19/200\n",
      "995000/995000 [==============================] - 540s 543us/step - loss: 2.5639 - acc: 0.2433 - val_loss: 2.5664 - val_acc: 0.2348\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.25840\n",
      "Epoch 20/200\n",
      "995000/995000 [==============================] - 540s 542us/step - loss: 2.5640 - acc: 0.2436 - val_loss: 2.5183 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.25840\n",
      "Epoch 21/200\n",
      "995000/995000 [==============================] - 539s 542us/step - loss: 2.5638 - acc: 0.2432 - val_loss: 2.5787 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.25840\n",
      "Epoch 22/200\n",
      "995000/995000 [==============================] - 542s 545us/step - loss: 2.5632 - acc: 0.2434 - val_loss: 2.5251 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.25840\n",
      "Epoch 23/200\n",
      "995000/995000 [==============================] - 544s 546us/step - loss: 2.5643 - acc: 0.2435 - val_loss: 2.5460 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.25840\n",
      "Epoch 24/200\n",
      "995000/995000 [==============================] - 544s 547us/step - loss: 2.5640 - acc: 0.2436 - val_loss: 2.5185 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.25840\n",
      "Epoch 25/200\n",
      "995000/995000 [==============================] - 542s 545us/step - loss: 2.5634 - acc: 0.2435 - val_loss: 2.5616 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.25840\n",
      "Epoch 26/200\n",
      "995000/995000 [==============================] - 538s 541us/step - loss: 2.5640 - acc: 0.2434 - val_loss: 2.5312 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.25840\n",
      "Epoch 27/200\n",
      "995000/995000 [==============================] - 540s 543us/step - loss: 2.5643 - acc: 0.2436 - val_loss: 2.5328 - val_acc: 0.2584\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.25840\n",
      "Epoch 28/200\n",
      "995000/995000 [==============================] - 541s 544us/step - loss: 2.5632 - acc: 0.2437 - val_loss: 2.5051 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.25840\n",
      "Epoch 29/200\n",
      "254400/995000 [======>.......................] - ETA: 6:42 - loss: 2.5613 - acc: 0.2431"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/bb-model-v3.1json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "fpath = \"weights/bb-v3.1/best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fpath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "BS = 100\n",
    "tb = TensorBoard(log_dir=\"tensorboard-logs/{}\".format('bb-modelv3.1'))\n",
    "callback_lst = [checkpoint, tb]\n",
    "#steps_per_epoch is num of batches that make up 1 epoch, defaults to size of train set\n",
    "model.fit(X,y,batch_size=BS, validation_split=.005, epochs=200, callbacks=callback_lst, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

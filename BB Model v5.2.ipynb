{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_bb_10k_comments.json') as json_file:  #first 1mill bb comments\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bb_tok = Tokenizer(oov_token='<unk>')\n",
    "bb_tok.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bb_tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bb_tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-04c9730d0e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bb-v5.2-tok.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bb_tok' is not defined"
     ]
    }
   ],
   "source": [
    "with open('bb-v5.2-tok.pickle', 'wb') as handle:\n",
    "    pickle.dump(bb_tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = data\n",
    "\n",
    "seqs = bb_tok.texts_to_sequences(train_data)\n",
    "less_20_seqs = list(filter(lambda x: len(x) <20, seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654993"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(less_20_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Len: 19\n"
     ]
    }
   ],
   "source": [
    "MAXLEN = 19\n",
    "VOCAB = int(len(bb_tok.word_index))\n",
    "SEED = 42\n",
    "\n",
    "sequences = pad_sequences(less_20_seqs, maxlen=MAXLEN, padding='pre')\n",
    "\n",
    "print('Max Seq Len: %d' % MAXLEN)\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.15, random_state=SEED)\n",
    "\n",
    "\n",
    "#y_cat = to_categorical(y, num_classes=vocab_sz) #1-hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to optimize this later, use np arrays instead of python lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, seqs took 57s to go through 1 mill samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           3000600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 500)               3204000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10002)             5011002   \n",
      "=================================================================\n",
      "Total params: 11,215,602\n",
      "Trainable params: 11,215,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#NN definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB, 300, input_length=MAXLEN-1))\n",
    "model.add(Bidirectional(LSTM(500),merge_mode='ave'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(VOCAB, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 551176 samples, validate on 5568 samples\n",
      "Epoch 1/200\n",
      "551176/551176 [==============================] - 730s 1ms/step - loss: nan - acc: 0.0431 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.04314, saving model to weights/huffpo-v5.2/best-weights.hdf5\n",
      "Epoch 2/200\n",
      "551176/551176 [==============================] - 735s 1ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.04314\n",
      "Epoch 3/200\n",
      "551176/551176 [==============================] - 776s 1ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.04314\n",
      "Epoch 4/200\n",
      "551176/551176 [==============================] - 773s 1ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.04314\n",
      "Epoch 5/200\n",
      "551176/551176 [==============================] - 752s 1ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.04314\n",
      "Epoch 6/200\n",
      "551176/551176 [==============================] - 727s 1ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.04314\n",
      "Epoch 7/200\n",
      "551176/551176 [==============================] - 838s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.04314\n",
      "Epoch 8/200\n",
      "551176/551176 [==============================] - 838s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.04314\n",
      "Epoch 9/200\n",
      "551176/551176 [==============================] - 839s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.04314\n",
      "Epoch 10/200\n",
      "551176/551176 [==============================] - 838s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.04314\n",
      "Epoch 11/200\n",
      "551176/551176 [==============================] - 839s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.04314\n",
      "Epoch 12/200\n",
      "551176/551176 [==============================] - 838s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00012: acc did not improve from 0.04314\n",
      "Epoch 13/200\n",
      "551176/551176 [==============================] - 839s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00013: acc did not improve from 0.04314\n",
      "Epoch 14/200\n",
      "551176/551176 [==============================] - 838s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00014: acc did not improve from 0.04314\n",
      "Epoch 15/200\n",
      "551176/551176 [==============================] - 839s 2ms/step - loss: nan - acc: 8.9082e-04 - val_loss: 9.2105 - val_acc: 5.3879e-04\n",
      "\n",
      "Epoch 00015: acc did not improve from 0.04314\n",
      "Epoch 16/200\n",
      "168200/551176 [========>.....................] - ETA: 9:40 - loss: 9.2105 - acc: 9.8098e-04"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/bb-model-v5.2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "fpath = \"weights/huffpo-v5.2/best-weights.hdf5\" #off of training acc\n",
    "checkpoint = ModelCheckpoint(fpath, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "BS = 100\n",
    "tb = TensorBoard(log_dir=\"tensorboard-logs/{}\".format('bb-modelv5.2-bilstm'))\n",
    "callback_lst = [checkpoint, tb]\n",
    "#steps_per_epoch is num of batches that make up 1 epoch, defaults to size of train set\n",
    "model.fit(X_train,y_train,batch_size=BS, validation_split=.01, epochs=200, callbacks=callback_lst, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

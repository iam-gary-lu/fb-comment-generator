{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  55873682\n",
      "elaspted text->int seqs:  47.3108069896698\n",
      "total seq construction time:  49.86976599693298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"bb_train<20.json\") as f:\n",
    "    all_data=json.load(f)\n",
    "print('num samples: ',len(all_data))\n",
    "\n",
    "\n",
    "#cant be too large, will trigger a memory error later on. Failed at 1 Mill\n",
    "#np.random.shuffle(all_data)\n",
    "\n",
    "data = all_data[:1000000]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Need to optimize this later, replace use np arrays \n",
    "#instead of python lists\n",
    "\n",
    "top_k = 10000\n",
    "\n",
    "t = Tokenizer(num_words=top_k, oov_token='<unk>')\n",
    "t.fit_on_texts(data)\n",
    "\"\"\"\n",
    "\n",
    "with open('bb-v1-tok.pickle', 'rb') as handle:\n",
    "    t = pickle.load(handle)\n",
    "    \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "seqs = [t.texts_to_sequences(x) for x in data]\n",
    "print('elaspted text->int seqs: ',time.time() - start)\n",
    "\n",
    "seq_data = list()\n",
    "for seq in seqs:\n",
    "    flat = [num for sublist in seq for num in sublist]\n",
    "    seq_data.append(flat)\n",
    "    \n",
    "print('total seq construction time: ',time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to optimize this later, use np arrays instead of python lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, seqs took 57s to go through 1 mill samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Len: 19\n",
      "vocab sz:  40497\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(seq) for seq in seq_data])\n",
    "\n",
    "sequences = pad_sequences(seq_data, maxlen=max_len, padding='pre')\n",
    "\n",
    "print('Max Seq Len: %d' % max_len)\n",
    "\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "vocab_sz = len(t.word_index)+1\n",
    "print('vocab sz: ',vocab_sz)\n",
    "\n",
    "#y_cat = to_categorical(y, num_classes=vocab_sz) #1-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 300)           12149100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40497)             8139897   \n",
      "=================================================================\n",
      "Total params: 20,689,797\n",
      "Trainable params: 20,689,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=7)\n",
    "\n",
    "#NN definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sz, 300, input_length=max_len-1))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(vocab_sz, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#save model structure as json\n",
    "\n",
    "#model_json = model.to_json()\n",
    "#with open(\"models/huffpo-model-v1.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dl-one/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 850000 samples, validate on 150000 samples\n",
      "Epoch 1/10\n",
      "850000/850000 [==============================] - 368s 433us/step - loss: 2.7910 - acc: 0.1798 - val_loss: 2.7702 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.77021, saving model to weights/bb-v2/best-weights.hdf5\n",
      "Epoch 2/10\n",
      "850000/850000 [==============================] - 362s 426us/step - loss: 2.7754 - acc: 0.1784 - val_loss: 2.7879 - val_acc: 0.1204\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.77021\n",
      "Epoch 3/10\n",
      "850000/850000 [==============================] - 362s 425us/step - loss: 2.7754 - acc: 0.1792 - val_loss: 2.7791 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.77021\n",
      "Epoch 4/10\n",
      "850000/850000 [==============================] - 365s 429us/step - loss: 2.7750 - acc: 0.1791 - val_loss: 2.7785 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.77021\n",
      "Epoch 5/10\n",
      "850000/850000 [==============================] - 362s 426us/step - loss: 2.7757 - acc: 0.1785 - val_loss: 2.7715 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.77021\n",
      "Epoch 6/10\n",
      "850000/850000 [==============================] - 361s 425us/step - loss: 2.7751 - acc: 0.1789 - val_loss: 2.7674 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.77021 to 2.76740, saving model to weights/bb-v2/best-weights.hdf5\n",
      "Epoch 7/10\n",
      "850000/850000 [==============================] - 362s 426us/step - loss: 2.7753 - acc: 0.1788 - val_loss: 2.7690 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.76740\n",
      "Epoch 8/10\n",
      "850000/850000 [==============================] - 363s 427us/step - loss: 2.7753 - acc: 0.1790 - val_loss: 2.7800 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.76740\n",
      "Epoch 9/10\n",
      "850000/850000 [==============================] - 361s 424us/step - loss: 2.7750 - acc: 0.1791 - val_loss: 2.7967 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.76740\n",
      "Epoch 10/10\n",
      "850000/850000 [==============================] - 362s 425us/step - loss: 2.7752 - acc: 0.1790 - val_loss: 2.7754 - val_acc: 0.1826\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.76740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6a6e44630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/bb-model-v2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "fpath = \"weights/bb-v2/best-weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(fpath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "BS = 100\n",
    "tb = TensorBoard(log_dir=\"tensorboard-logs/{}\".format('bb-modelv2'))\n",
    "callback_lst = [checkpoint, tb]\n",
    "#steps_per_epoch is num of batches that make up 1 epoch, defaults to size of train set\n",
    "model.fit(X,y,batch_size=BS, validation_split=.15, epochs=10, callbacks=callback_lst, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
